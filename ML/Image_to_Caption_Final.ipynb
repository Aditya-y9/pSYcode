{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WuKRVA2EM1s5"
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ZZPdpqyPI2L",
    "outputId": "63219035-f866-4f62-9cea-527190ea720d"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import (BlipProcessor, BlipForConditionalGeneration, \n",
    "                          AutoTokenizer, AutoModelForSeq2SeqLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SBq0_-yDRawH"
   },
   "outputs": [],
   "source": [
    "t5_tokenizer = AutoTokenizer.from_pretrained('text-to-social-media-captions')\n",
    "t5_model = AutoModelForSeq2SeqLM.from_pretrained('text-to-social-media-captions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "77KL--FOU46C"
   },
   "outputs": [],
   "source": [
    "def generate_caption_for_img(img_link='https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg',text = \"\",url=False,img_show=True):\n",
    "    \n",
    "    img_desc = generate_desc(img_link,text,url)\n",
    "    if img_show==True:\n",
    "        if url==True:\n",
    "            Image.open(requests.get(img_link, stream=True).raw).show()\n",
    "        else:\n",
    "            Image.open(img_link).show()\n",
    "\n",
    "    print('Image Description:',img_desc)\n",
    "    \n",
    "    caption = generate_caption(img_desc)\n",
    "    print('Caption:',caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 693
    },
    "id": "Y4xeV4fZU-2E",
    "outputId": "63d2ab07-cf54-426e-daf9-f769e522af4e"
   },
   "outputs": [],
   "source": [
    "generate_caption_for_img(url=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QS8HXiPiV6MN"
   },
   "outputs": [],
   "source": [
    "img_link = fr\"C:\\Users\\sgp\\Downloads\\Image1.png\".replace('\\\\','/')\n",
    "generate_caption_for_img(img_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app.py\n",
    "import streamlit as st\n",
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import (BlipProcessor, BlipForConditionalGeneration, \n",
    "                          AutoTokenizer, AutoModelForSeq2SeqLM)\n",
    "\n",
    "#Load the models\n",
    "@st.cache_resource\n",
    "def get_models():\n",
    "    blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "    blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "    t5_tokenizer = AutoTokenizer.from_pretrained('prasanthsagirala/text-to-social-media-captions')\n",
    "    t5_model = AutoModelForSeq2SeqLM.from_pretrained('prasanthsagirala/text-to-social-media-captions')\n",
    "    return blip_processor,blip_model,t5_tokenizer,t5_model\n",
    "\n",
    "blip_processor,blip_model,t5_tokenizer,t5_model = get_models()\n",
    "\n",
    "#get the description of image\n",
    "def generate_desc(img_url):\n",
    "    raw_image = Image.open(img_url).convert('RGB')\n",
    "    inputs = blip_processor(raw_image, return_tensors=\"pt\")\n",
    "    out = blip_model.generate(**inputs)\n",
    "    return(blip_processor.decode(out[0], skip_special_tokens=True))\n",
    "    \n",
    "#get the caption for the description\n",
    "def generate_caption(text):\n",
    "    inputs = [\"captionize: \" + text]\n",
    "    inputs = t5_tokenizer(inputs, max_length=512, truncation=True, return_tensors=\"pt\")\n",
    "    output = t5_model.generate(**inputs, num_beams=8, do_sample=True, min_length=10, max_length=64)\n",
    "    decoded_output = t5_tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
    "    return decoded_output\n",
    "\n",
    "#get the caption for the image\n",
    "def generate_caption_for_img(img_link):    \n",
    "    img_desc = generate_desc(img_link)\n",
    "    caption = generate_caption(img_desc)\n",
    "    return img_desc,caption\n",
    "\n",
    "#User Input\n",
    "def main():\n",
    "    st.title('Caption Generator from Image')\n",
    "    image_file = st.file_uploader(\"Upload Images\", type=[\"png\",\"jpg\",\"jpeg\"])\n",
    "    if image_file!=None:\n",
    "        st.image(Image.open(image_file),width=250)\n",
    "        if st.button('Captionize'):\n",
    "            img_desc = generate_desc(image_file)\n",
    "            st.write('Image Description: ',img_desc)\n",
    "            for i in range(5):\n",
    "                st.write(f'Caption-{i+1}: ',generate_caption(img_desc))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
